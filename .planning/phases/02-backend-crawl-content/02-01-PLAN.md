---
phase: 02-backend-crawl-content
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - site-audit/skills/site-audit/SKILL.md
  - site-audit/skills/site-audit/references/URL_RULES.md
autonomous: true

must_haves:
  truths:
    - "Plugin discovers all same-domain pages reachable from seed URL"
    - "Same page never visited twice regardless of URL format variations"
    - "Crawl terminates gracefully at page cap without infinite loops"
    - "Crawl state (queue, visited, count) is explicitly tracked throughout"
  artifacts:
    - path: "site-audit/skills/site-audit/SKILL.md"
      provides: "BFS crawl loop implementation in Phase 2"
      contains: "Phase 2: Crawl Loop"
      min_lines: 100
    - path: "site-audit/skills/site-audit/references/URL_RULES.md"
      provides: "URL normalization rules per RFC 3986"
      contains: "Lowercase scheme and host"
      min_lines: 30
  key_links:
    - from: "SKILL.md Phase 2"
      to: "references/URL_RULES.md"
      via: "explicit reference to normalization rules"
      pattern: "@references/URL_RULES\\.md"
    - from: "SKILL.md crawl loop"
      to: "WebFetch tool"
      via: "link extraction on each page"
      pattern: "WebFetch.*extract.*links"
    - from: "SKILL.md queue processing"
      to: "visited set"
      via: "duplicate check before adding URL"
      pattern: "in visited|visited set"
---

<objective>
Implement the core BFS crawl infrastructure that discovers and visits all same-domain pages from a seed URL.

Purpose: Establish the foundational crawl loop that will enable content analysis and dead link detection. This provides the page discovery mechanism that all subsequent analysis depends on.

Output: SKILL.md Phase 2 with working BFS crawl loop + URL normalization reference file
</objective>

<execution_context>
@/Users/sivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/PROJECT.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/ROADMAP.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/STATE.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/REQUIREMENTS.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/phases/02-backend-crawl-content/02-RESEARCH.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/site-audit/skills/site-audit/SKILL.md
</context>

<tasks>

<task type="auto">
  <name>Create URL normalization reference file</name>
  <files>site-audit/skills/site-audit/references/URL_RULES.md</files>
  <action>
  Create references subdirectory and URL_RULES.md file containing explicit URL normalization rules per RFC 3986.

  Include these normalization rules (from 02-RESEARCH.md Pattern 4):
  - Lowercase scheme and host (path stays case-sensitive)
  - Remove fragment identifiers (#section)
  - Remove trailing slash for non-root paths
  - Keep root trailing slash (https://example.com/)
  - Remove default ports (443 for https, 80 for http)
  - Decode unreserved percent-encoded characters
  - Prefer HTTPS when both protocols encountered

  Add same-domain check rules:
  - Extract hostname from normalized URL
  - Strict hostname matching (www.example.com ≠ example.com)
  - If seed has www., only www.example.com matches

  Add URL skip rules:
  - Skip mailto:, tel:, javascript: schemes
  - Skip asset extensions: .pdf, .jpg, .png, .gif, .css, .js, .ico
  - Skip fragment-only links (#section)

  Format as clear markdown checklist that Claude can follow during crawl loop.
  </action>
  <verify>
  File exists at site-audit/skills/site-audit/references/URL_RULES.md
  Contains all 7 normalization rules
  Contains same-domain check rules
  Contains URL skip rules
  </verify>
  <done>URL_RULES.md exists with complete normalization, same-domain, and skip rules as markdown checklist</done>
</task>

<task type="auto">
  <name>Implement BFS crawl loop in SKILL.md Phase 2</name>
  <files>site-audit/skills/site-audit/SKILL.md</files>
  <action>
  Replace the "Phase 2: Crawl (to be implemented)" placeholder section with a complete BFS crawl implementation.

  Follow the BFS pattern from 02-RESEARCH.md (Pattern 2, lines 379-441):

  Structure as:
  **Phase 2: Initialization**
  - Parse seed URL (protocol, domain, path)
  - Initialize crawl state (queue, visited, page_count, max_pages=50, external_links)
  - Create .audit-data directory for findings
  - Reference @references/URL_RULES.md for normalization
  - Confirm with user before starting

  **Phase 2: Crawl Loop**
  - Termination check: queue empty OR page_count >= max_pages → exit
  - Dequeue next URL from front (FIFO)
  - Check if URL in visited set → skip if yes
  - Add to visited, increment page_count
  - WebFetch with link extraction prompt (from 02-RESEARCH.md line 485-497)
  - Parse extracted links, normalize each per URL_RULES.md
  - Classify: same-domain (enqueue) vs external (record) vs skip
  - State update every page: "Page [N]/[max]: [url] - Queue: [Y] URLs"
  - Full state every 5 pages: visited count, next 3 URLs in queue
  - Loop back to termination check

  DO NOT implement content analysis yet (spelling, dead links) - that's Plan 02-02. This task focuses solely on page discovery and visiting.

  Key implementation details:
  - Use WebFetch prompt requesting [text](url) format for easy parsing
  - Explicit state declaration after each page processed
  - Normalize URLs BEFORE adding to queue or visited
  - Track external links in separate list (for future verification)

  Keep Phase 2 under 150 lines total (init + loop). This is skill writing, not code - be clear and instructive, not verbose.
  </action>
  <verify>
  SKILL.md contains "Phase 2: Initialization" section
  SKILL.md contains "Phase 2: Crawl Loop" section
  Crawl loop includes termination check (queue empty OR page cap)
  WebFetch call with link extraction prompt present
  URL normalization step references URL_RULES.md
  State declaration instructions present (every page + every 5 pages)
  Visited set duplicate check present
  BFS queue FIFO behavior specified
  </verify>
  <done>SKILL.md Phase 2 replaced with complete BFS crawl loop implementation covering CRAWL-01, CRAWL-02, CRAWL-03, CRAWL-04</done>
</task>

</tasks>

<verification>
**Overall phase checks:**

1. Read SKILL.md Phase 2 section - verify it contains BFS crawl loop with:
   - Initialization with crawl state
   - Termination conditions (queue empty OR page cap)
   - FIFO queue processing
   - Visited set duplicate prevention
   - WebFetch link extraction
   - URL normalization step

2. Read URL_RULES.md - verify it contains:
   - All 7 normalization rules from RFC 3986
   - Same-domain check rules
   - URL skip rules (mailto, tel, assets)

3. Grep for @references/URL_RULES.md in SKILL.md - verify Phase 2 references it

4. Count lines in SKILL.md Phase 2 section - should be ~100-150 lines (concise skill writing)

5. Verify Phase 3, 4, 5 placeholders still exist in SKILL.md (we only replaced Phase 2)
</verification>

<success_criteria>
**Measurable completion:**

- [ ] site-audit/skills/site-audit/references/ directory exists
- [ ] URL_RULES.md file exists with 7 normalization rules + skip rules
- [ ] SKILL.md Phase 2 no longer says "to be implemented"
- [ ] SKILL.md Phase 2 contains BFS crawl loop with 10+ explicit steps
- [ ] SKILL.md references @references/URL_RULES.md
- [ ] Crawl loop has termination check for page cap
- [ ] Crawl loop has visited set duplicate prevention
- [ ] WebFetch link extraction prompt is specified
- [ ] State declaration instructions present
- [ ] Phase 2 section is under 200 lines total
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-crawl-content/02-01-SUMMARY.md`
</output>
