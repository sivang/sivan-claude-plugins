---
phase: 02-backend-crawl-content
plan: 02
type: execute
wave: 2
depends_on:
  - 02-01
files_modified:
  - site-audit/skills/site-audit/SKILL.md
  - site-audit/skills/site-audit/references/CHECKS.md
autonomous: true

must_haves:
  truths:
    - "Dead internal links (404s, errors) are detected and recorded"
    - "Spelling/grammar issues are identified on page text"
    - "Code blocks and technical terms are filtered from spell-check"
    - "Findings are written to disk progressively during crawl"
  artifacts:
    - path: "site-audit/skills/site-audit/SKILL.md"
      provides: "Content analysis within crawl loop"
      contains: "spelling|grammar|broken.*link"
      min_lines: 150
    - path: "site-audit/skills/site-audit/references/CHECKS.md"
      provides: "Detailed analysis rules for content checks"
      contains: "spelling|dead link"
      min_lines: 40
  key_links:
    - from: "SKILL.md crawl loop"
      to: "WebFetch page content"
      via: "extracts text for spelling analysis"
      pattern: "main text content|page text"
    - from: "SKILL.md analysis"
      to: ".audit-data/*.jsonl files"
      via: "writes findings to JSONL"
      pattern: "echo.*\\.audit-data.*jsonl"
    - from: "SKILL.md crawl loop"
      to: "references/CHECKS.md"
      via: "references analysis rules"
      pattern: "@references/CHECKS\\.md"
    - from: "WebFetch response"
      to: "dead link detection"
      via: "error handling for 404/timeouts"
      pattern: "WebFetch.*error|timeout|404"
---

<objective>
Add content analysis capabilities to the crawl loop: dead link detection and AI-powered spelling/grammar checking with technical content filtering.

Purpose: Layer quality checks onto the page discovery mechanism from Plan 02-01. This completes Phase 2's backend analysis capabilities before moving to Chrome-based UI checks in Phase 3.

Output: SKILL.md with content analysis integrated into crawl loop + CHECKS.md reference file with filtering rules
</objective>

<execution_context>
@/Users/sivan/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sivan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/PROJECT.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/ROADMAP.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/REQUIREMENTS.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/phases/02-backend-crawl-content/02-RESEARCH.md
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/site-audit/skills/site-audit/SKILL.md

# Reference 02-01 outputs
@/Users/sivan/VitakkaProjects/sivan-claude-plugins/.planning/phases/02-backend-crawl-content/02-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Create content analysis reference file</name>
  <files>site-audit/skills/site-audit/references/CHECKS.md</files>
  <action>
  Create CHECKS.md with detailed rules for spelling/grammar analysis and dead link detection.

  **Spelling/Grammar Analysis Section:**
  - Analyze extracted page text for typos and grammar issues
  - Filter out: code blocks (indented text, ```fenced blocks), URLs, email addresses
  - Filter out: camelCase identifiers, SCREAMING_CASE, snake_case (likely technical terms)
  - Filter out: Capitalized words that may be brand/product names
  - Only flag HIGH confidence spelling errors (obvious typos like "occured" â†’ "occurred")
  - Context-aware: consider surrounding words to avoid jargon false positives
  - Provide suggestion and context snippet for each finding

  **Dead Link Detection Section:**
  - Dead links are internal same-domain URLs that return errors
  - WebFetch errors indicate dead links: 404, timeout, connection refused, DNS errors
  - Record: source page, broken link URL, error type
  - Do NOT verify external links during crawl (deferred to separate phase)

  **Finding Format:**
  - Each finding is a single-line JSON object
  - Timestamp in ISO 8601 format
  - Example formats from 02-RESEARCH.md lines 509-518

  **Progressive Writing:**
  - Write findings to .audit-data/ directory as JSONL
  - One file per finding type: findings-spelling.jsonl, findings-broken-links.jsonl
  - Append-only writes (echo >> file)
  - Write after EACH page analysis, before moving to next page

  Format as clear markdown sections with examples.
  </action>
  <verify>
  File exists at site-audit/skills/site-audit/references/CHECKS.md
  Contains "Spelling/Grammar Analysis" section with filtering rules
  Contains "Dead Link Detection" section
  Contains JSONL format examples
  Contains progressive writing instructions
  </verify>
  <done>CHECKS.md exists with complete analysis rules, filtering instructions, and JSONL format specifications</done>
</task>

<task type="auto">
  <name>Integrate content analysis into SKILL.md crawl loop</name>
  <files>site-audit/skills/site-audit/SKILL.md</files>
  <action>
  Enhance the existing Phase 2 crawl loop (from 02-01) to include content analysis after each WebFetch.

  Modify the WebFetch step (currently just extracting links) to ALSO request page text:
  - Update WebFetch prompt to request: "1) All links in [text](url) format, one per line. 2) Main text content of the page (skip code blocks, navigation, footer)."
  - This gets both links (for crawl) and content (for analysis) in ONE WebFetch call

  Add new step after WebFetch, before "Process extracted links":

  **5. Analyze page content (reference @references/CHECKS.md)**
  - Dead link detection: Did WebFetch return an error for this page?
    - If yes: Record as broken link (source = referring page, target = current URL, error = type)
  - Spelling/grammar check: Analyze the returned page text
    - Apply filtering rules from CHECKS.md (ignore code, technical terms, brands)
    - Identify HIGH confidence spelling/grammar errors
    - For each error: extract context snippet, suggest correction

  **6. Write findings to JSONL files**
  - If broken link detected:
    ```bash
    echo '{"type":"broken_link","page":"[source]","target":"[url]","error":"[type]","timestamp":"[iso8601]"}' >> .audit-data/findings-broken-links.jsonl
    ```
  - For each spelling issue:
    ```bash
    echo '{"type":"spelling","page":"[url]","word":"[word]","suggestion":"[fix]","context":"[snippet]","timestamp":"[iso8601]"}' >> .audit-data/findings-spelling.jsonl
    ```

  Update the initialization section to create .audit-data directory and empty JSONL files:
  ```bash
  mkdir -p .audit-data
  touch .audit-data/findings-broken-links.jsonl
  touch .audit-data/findings-spelling.jsonl
  ```

  Update state declaration to include findings count:
  - Every page: "Page [N]: [url] - [X] issues found (broken links: [B], spelling: [S]). Queue: [Y]."
  - Every 5 pages: Include total findings by type

  IMPORTANT: Dead link detection happens in TWO places:
  1. When WebFetch FAILS to fetch a page (target page is dead)
  2. When WebFetch succeeds but extracted links point to URLs that later fail when fetched

  The first case (WebFetch failure) is detected immediately when trying to fetch the page.
  The second case is detected when those linked pages are dequeued and fetched later in the loop.

  Keep total Phase 2 under 250 lines. Be concise - this is instructive markdown, not verbose documentation.
  </action>
  <verify>
  SKILL.md Phase 2 Initialization creates .audit-data directory and JSONL files
  WebFetch prompt now requests both links AND page text content
  Analysis step exists after WebFetch with spelling and dead link checks
  JSONL writing bash commands present with correct format
  State declaration includes findings counts
  References @references/CHECKS.md for analysis rules
  Total Phase 2 section is under 300 lines
  </verify>
  <done>SKILL.md Phase 2 enhanced with content analysis covering CRAWL-05, CONT-01, CONT-02; findings written progressively to JSONL</done>
</task>

</tasks>

<verification>
**Overall phase checks:**

1. Read CHECKS.md - verify it contains:
   - Spelling/grammar filtering rules (code, technical terms, brands)
   - Dead link detection rules
   - JSONL format examples
   - Progressive writing instructions

2. Read SKILL.md Phase 2 - verify additions:
   - WebFetch prompt requests both links AND page text
   - Analysis step after WebFetch
   - Spelling check with filtering
   - Dead link detection for WebFetch errors
   - JSONL writing commands (echo >> .audit-data/findings-*.jsonl)
   - State declaration includes findings counts

3. Verify SKILL.md references both:
   - @references/URL_RULES.md (from 02-01)
   - @references/CHECKS.md (from this plan)

4. Check SKILL.md line count for Phase 2 - should be 150-250 lines total (concise)

5. Verify Phases 3, 4, 5 still have placeholders (we only modified Phase 2)
</verification>

<success_criteria>
**Measurable completion:**

- [ ] CHECKS.md file exists with spelling and dead link analysis rules
- [ ] SKILL.md Phase 2 Initialization creates .audit-data directory
- [ ] WebFetch prompt requests links + page text content
- [ ] Analysis step present after WebFetch
- [ ] Spelling filtering rules applied (code blocks, technical terms, brands)
- [ ] Dead link detection for WebFetch errors
- [ ] JSONL writing bash commands present for both finding types
- [ ] State declaration includes findings counts
- [ ] SKILL.md references @references/CHECKS.md
- [ ] Phase 2 section is under 300 lines total
- [ ] All 7 Phase 2 requirements covered: CRAWL-01 through CRAWL-05, CONT-01, CONT-02
</success_criteria>

<output>
After completion, create `.planning/phases/02-backend-crawl-content/02-02-SUMMARY.md`
</output>
